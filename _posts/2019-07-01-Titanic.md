---
layout: post
title:  "Kaggle: Titanic Disaster (Accuracy 79.0%)"
date:   2019-07-06
excerpt: "A take on the Kaggle competition of the Titanic disaster."
project: true
tag:
- Kaggle 
- ML
- python
comments: False
---

<footer id="attribution" style="float:right; color:#999; background:#fff;">
Created by Thibault Dody, 07/05/2019.
</footer>

# Titanic Disaster Study

<figure>
<img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/Titanic1.jpg" style="width:642px;height=288px;">
</figure>

## Table of Content   

[**1. Introduction**](#Section_1)   
[**2. Data Import**](#Section_2)  
&nbsp;&nbsp;&nbsp;&nbsp;[2.1 Import Libraries](#Section_21)  
&nbsp;&nbsp;&nbsp;&nbsp;[2.2 Load specific tools](#Section_22)  
&nbsp;&nbsp;&nbsp;&nbsp;[2.3 Data Import](#Section_23)  
&nbsp;&nbsp;&nbsp;&nbsp;[2.4 Data Inspection](#Section_24)  
[**3. Data Exploration and Data Cleaning**](#Section_3)  
&nbsp;&nbsp;&nbsp;&nbsp;[3.1 Pivoting Features](#Section_31)  
&nbsp;&nbsp;&nbsp;&nbsp;[3.2 Embarked Feature](#Section_32)  
&nbsp;&nbsp;&nbsp;&nbsp;[3.3 Fare Feature](#Section_33)  
&nbsp;&nbsp;&nbsp;&nbsp;[3.4 Cabin Feature](#Section_34)  
&nbsp;&nbsp;&nbsp;&nbsp;[3.5 Age Feature](#Section_35)  
[**4. Data Visualization and Feature Exploration**](#Section_4)  
&nbsp;&nbsp;&nbsp;&nbsp;[4.1 Gender](#Section_41)  
&nbsp;&nbsp;&nbsp;&nbsp;[4.2 Age](#Section_42)  
&nbsp;&nbsp;&nbsp;&nbsp;[4.3 Pclass and Fare](#Section_43)  
&nbsp;&nbsp;&nbsp;&nbsp;[4.4 SibSp & Parch](#Section_44)  
&nbsp;&nbsp;&nbsp;&nbsp;[4.5 Embarked](#Section_45)  
[**5. Statistical Study**](#Section_5)  
&nbsp;&nbsp;&nbsp;&nbsp;[5.1 Main Features](#Section_51)  
&nbsp;&nbsp;&nbsp;&nbsp;[5.2 Correlation Study](#Section_52)  
[**6. Feature Engineering**](#Section_6)  
&nbsp;&nbsp;&nbsp;&nbsp;[6.1 Name](#Section_61)  
&nbsp;&nbsp;&nbsp;&nbsp;[6.2 Ticket](#Section_62)  
&nbsp;&nbsp;&nbsp;&nbsp;[6.3 Fare](#Section_63)  
&nbsp;&nbsp;&nbsp;&nbsp;[6.4 Cabin](#Section_64)  
[**7. Cleaning Functions**](#Section_7)  
[**8. Model Preparation**](#Section_8)  
[**9. Models**](#Section_9)  
[**10. Ensemble**](#Section_10)  
[**11. Create Submission**](#Section_11)  

<a id="Section_1"></a>
## 1. Introduction

On April 15, 1912, the Titanic sunk after colliding with an iceberg, 1502 out of 2224 passenger and crew members died. The dataset containing passenger information has been made available. The purpose of this Notebook is to perform a comparison study of different models aimed at predicting survival rate. The data is obtained from [Kaggle](https://www.kaggle.com/).

*****
<a id="Section_2"></a>
## 2. Data Import
<a id="Section_21"></a>
### 2.1 Import Libraries


```python
# Load libraries
import sys
print("Python version:\t\t{}".format(sys.version))

import pandas as pd
print("pandas version:\t\t{}".format(pd.__version__))

import matplotlib
print("matplotlib version:\t{}".format(matplotlib.__version__))

import numpy as np
print("numpy version:\t\t{}".format(np.__version__))

import scipy as sp
print("scipy version:\t\t{}".format(sp.__version__))

import sklearn
print("sklearn version:\t{}".format(sklearn.__version__))

import fancyimpute
from fancyimpute import KNN as KNN_fi
```

    Python version:		3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) 
    [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]
    pandas version:		0.24.2
    matplotlib version:	3.1.0
    numpy version:		1.16.4
    scipy version:		1.2.1
    sklearn version:	0.21.2


    Using TensorFlow backend.


<a id="Section_22"></a>
### 2.2 Load specific tools


```python
# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="whitegrid")
%matplotlib inline

# Models
from sklearn.model_selection import cross_val_score,GridSearchCV, StratifiedKFold, train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, ExtraTreeClassifier
from sklearn.naive_bayes import GaussianNB, BernoulliNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC, NuSVC, LinearSVC
from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier, SGDClassifier, Perceptron
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import AdaBoostClassifier, VotingClassifier, ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier
from sklearn.gaussian_process import GaussianProcessClassifier

from xgboost import XGBClassifier

# Tools
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score,classification_report, precision_recall_curve, confusion_matrix
from sklearn.model_selection import cross_val_score, GridSearchCV

from collections import Counter

# Do not show warnings (added after Notebook was finalized)
import warnings
warnings.filterwarnings('ignore')
```

<a id="Section_23"></a>
### 2.3 Data Import


```python
# Import training and testing csv datasets
train = pd.read_csv('./Data/train.csv')
test = pd.read_csv('./Data/test.csv')
```


```python
# Inspect data
train.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Inspect data
test.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>892</td>
      <td>3</td>
      <td>Kelly, Mr. James</td>
      <td>male</td>
      <td>34.5</td>
      <td>0</td>
      <td>0</td>
      <td>330911</td>
      <td>7.8292</td>
      <td>NaN</td>
      <td>Q</td>
    </tr>
    <tr>
      <th>1</th>
      <td>893</td>
      <td>3</td>
      <td>Wilkes, Mrs. James (Ellen Needs)</td>
      <td>female</td>
      <td>47.0</td>
      <td>1</td>
      <td>0</td>
      <td>363272</td>
      <td>7.0000</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>2</th>
      <td>894</td>
      <td>2</td>
      <td>Myles, Mr. Thomas Francis</td>
      <td>male</td>
      <td>62.0</td>
      <td>0</td>
      <td>0</td>
      <td>240276</td>
      <td>9.6875</td>
      <td>NaN</td>
      <td>Q</td>
    </tr>
    <tr>
      <th>3</th>
      <td>895</td>
      <td>3</td>
      <td>Wirz, Mr. Albert</td>
      <td>male</td>
      <td>27.0</td>
      <td>0</td>
      <td>0</td>
      <td>315154</td>
      <td>8.6625</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>896</td>
      <td>3</td>
      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>
      <td>female</td>
      <td>22.0</td>
      <td>1</td>
      <td>1</td>
      <td>3101298</td>
      <td>12.2875</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>



<a id="Section_24"></a>
### 2.4 Data Inspection

The data is divided into two separate datasets:
- a training set containing a set of features and out target variable (whether or not a passenger survived)
- a test set containing only the set of features

#### Data  Features
**. Pclass**: Categorical feature used to describe the passenger class (1=Upper, 2=Middle, 3=Lower).   
**. Name**: String Containing a passenger name and title.   
**. Sex**: Categorical variable describing the passenger's gender.   
**. Age**: Numerical feature standing for the passenger's age.   
**. SibSp**: Number of siblings/spouses aboard.   
**. Parch**: Number of parents/children aboard.   
**. Ticket**: Ticket number.   
**. Fare**: Price of the ticket.   
**. Cabin**: Cabin id.   
**. Embarked**: Categorical feature, port of embarkation.  
  
**. Survived**: Target feature (1=Survived, 0=Died)


```python
# Outliers detections
def detect_outliers(df, features, n):
    '''
    Returns outliers records from df. The method used is based on the interquartile range [Q1-1.5*IQR, Q3+1.5*IQR].
    
    inputs:
    df--input dataframe
    features--list of features to be considered
    n--integer, if an record appears as outliers in n or more features, it is returned as outlier.
    
    outputs:
    outliers--dataframe containing the outliers
    '''
    
    # prepare storage for outlier indexes
    outliers_idx = []
    
    # iterate over features
    for feature in features:
        
        # Compute 1st (25%) and 3rd quartile
        Q1 = np.percentile(df[feature], 25)
        Q3 = np.percentile(df[feature], 75)
        
        # Compute IQR
        IQR = Q3 - Q1
        
        # Compute lower bound and upper bound
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        
        # print
        # print(feature, lower, upper)
        
        # isolate indexes corresponding to records outside the range
        feature_outlier_idx = df[(df[feature]<lower) | (df[feature]>upper)].index
        
        # store outlier indexes
        outliers_idx.extend(feature_outlier_idx)
        
    # count outlier occurences
    outliers_idx_count = Counter(outliers_idx)
    
    # filter outliers based on n
    outliers_idx = [idx for idx, val in outliers_idx_count.items() if val > n]
    
    # return outliers
    return outliers_idx
```


```python
print("Identified outliers from training set:")
train.loc[detect_outliers(train, ['SibSp', 'Parch', 'Fare'], 2)]
```

    Identified outliers from training set:





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>27</th>
      <td>28</td>
      <td>0</td>
      <td>1</td>
      <td>Fortune, Mr. Charles Alexander</td>
      <td>male</td>
      <td>19.0</td>
      <td>3</td>
      <td>2</td>
      <td>19950</td>
      <td>263.00</td>
      <td>C23 C25 C27</td>
      <td>S</td>
    </tr>
    <tr>
      <th>88</th>
      <td>89</td>
      <td>1</td>
      <td>1</td>
      <td>Fortune, Miss. Mabel Helen</td>
      <td>female</td>
      <td>23.0</td>
      <td>3</td>
      <td>2</td>
      <td>19950</td>
      <td>263.00</td>
      <td>C23 C25 C27</td>
      <td>S</td>
    </tr>
    <tr>
      <th>159</th>
      <td>160</td>
      <td>0</td>
      <td>3</td>
      <td>Sage, Master. Thomas Henry</td>
      <td>male</td>
      <td>NaN</td>
      <td>8</td>
      <td>2</td>
      <td>CA. 2343</td>
      <td>69.55</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>180</th>
      <td>181</td>
      <td>0</td>
      <td>3</td>
      <td>Sage, Miss. Constance Gladys</td>
      <td>female</td>
      <td>NaN</td>
      <td>8</td>
      <td>2</td>
      <td>CA. 2343</td>
      <td>69.55</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>201</th>
      <td>202</td>
      <td>0</td>
      <td>3</td>
      <td>Sage, Mr. Frederick</td>
      <td>male</td>
      <td>NaN</td>
      <td>8</td>
      <td>2</td>
      <td>CA. 2343</td>
      <td>69.55</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>324</th>
      <td>325</td>
      <td>0</td>
      <td>3</td>
      <td>Sage, Mr. George John Jr</td>
      <td>male</td>
      <td>NaN</td>
      <td>8</td>
      <td>2</td>
      <td>CA. 2343</td>
      <td>69.55</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>341</th>
      <td>342</td>
      <td>1</td>
      <td>1</td>
      <td>Fortune, Miss. Alice Elizabeth</td>
      <td>female</td>
      <td>24.0</td>
      <td>3</td>
      <td>2</td>
      <td>19950</td>
      <td>263.00</td>
      <td>C23 C25 C27</td>
      <td>S</td>
    </tr>
    <tr>
      <th>792</th>
      <td>793</td>
      <td>0</td>
      <td>3</td>
      <td>Sage, Miss. Stella Anna</td>
      <td>female</td>
      <td>NaN</td>
      <td>8</td>
      <td>2</td>
      <td>CA. 2343</td>
      <td>69.55</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>846</th>
      <td>847</td>
      <td>0</td>
      <td>3</td>
      <td>Sage, Mr. Douglas Bullen</td>
      <td>male</td>
      <td>NaN</td>
      <td>8</td>
      <td>2</td>
      <td>CA. 2343</td>
      <td>69.55</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>863</th>
      <td>864</td>
      <td>0</td>
      <td>3</td>
      <td>Sage, Miss. Dorothy Edith "Dolly"</td>
      <td>female</td>
      <td>NaN</td>
      <td>8</td>
      <td>2</td>
      <td>CA. 2343</td>
      <td>69.55</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>



From the above table, three outliers have very have fare while the other have a high number corresponding to the SibSp feature. Since outliers can have a significant negative impact on the model, these records are dropped from the training set.


```python
def del_outliers(df, outliers_idx):
    df = df.drop(outliers_idx, axis=0)
    return df
```


```python
train = del_outliers(train, detect_outliers(train, ['SibSp', 'Parch', 'Fare'], 2))
```

****
<a id="Section_3"></a>
## 3. Data Exploration and Data Cleaning


```python
# Remove the Passengerid from the set as it does not need to be included in the models
passengerId = test.PassengerId

train = train.drop(['PassengerId'],axis=1)
test = test.drop(['PassengerId'],axis=1)
```


```python
# list information for each feature (type, number of nun-null records)
train.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Int64Index: 881 entries, 0 to 890
    Data columns (total 11 columns):
    Survived    881 non-null int64
    Pclass      881 non-null int64
    Name        881 non-null object
    Sex         881 non-null object
    Age         711 non-null float64
    SibSp       881 non-null int64
    Parch       881 non-null int64
    Ticket      881 non-null object
    Fare        881 non-null float64
    Cabin       201 non-null object
    Embarked    879 non-null object
    dtypes: float64(2), int64(4), object(5)
    memory usage: 82.6+ KB



```python
# list information for each feature (type, number of nun-null records)
test.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 418 entries, 0 to 417
    Data columns (total 10 columns):
    Pclass      418 non-null int64
    Name        418 non-null object
    Sex         418 non-null object
    Age         332 non-null float64
    SibSp       418 non-null int64
    Parch       418 non-null int64
    Ticket      418 non-null object
    Fare        417 non-null float64
    Cabin       91 non-null object
    Embarked    418 non-null object
    dtypes: float64(2), int64(3), object(5)
    memory usage: 32.7+ KB


**Comment**:   
Several of the features in the **training** set appear to be incomplete (Age, Cabin, and Embarked).   
Several of the features in the **test** set appear to be incomplete (Age, Cabin, and Fare).


```python
# compute percentage of missing values

# compute number of missing records
missing_total = train.isnull().sum().sort_values(ascending=False)

# convert to percentages
missing_percentage = missing_total/train.shape[0]*100

# display missing record %
print('Missing values in training set:')
pd.concat([missing_total,missing_percentage],keys=['Count','Percentage'],axis=1)
```

    Missing values in training set:





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Count</th>
      <th>Percentage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Cabin</th>
      <td>680</td>
      <td>77.185017</td>
    </tr>
    <tr>
      <th>Age</th>
      <td>170</td>
      <td>19.296254</td>
    </tr>
    <tr>
      <th>Embarked</th>
      <td>2</td>
      <td>0.227015</td>
    </tr>
    <tr>
      <th>Fare</th>
      <td>0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Ticket</th>
      <td>0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Parch</th>
      <td>0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>SibSp</th>
      <td>0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Sex</th>
      <td>0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Name</th>
      <td>0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Pclass</th>
      <td>0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Survived</th>
      <td>0</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>



**Comment**:  
Based on the above table, the following observations can be made:
1. The cabin feature is mostly empty, this will be hard to use.
2. The age feature contains a large number of missing values. This will require a smarter approach rather than just filling the null with a median.
3. The embarked feature only has 2 missing values. We can come up with estimates for these two by taking a quick look at the data and using the most probable values as replacements.


```python
# compute percentage of missing values

# compute number of missing records
missing_total = test.isnull().sum().sort_values(ascending=False)

# convert to percentages
missing_percentage = missing_total/train.shape[0]*100

# display missing record %
print('Missing values in test set:')
pd.concat([missing_total,missing_percentage],keys=['Count','Percentage'],axis=1)
```

    Missing values in test set:





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Count</th>
      <th>Percentage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Cabin</th>
      <td>327</td>
      <td>37.116913</td>
    </tr>
    <tr>
      <th>Age</th>
      <td>86</td>
      <td>9.761635</td>
    </tr>
    <tr>
      <th>Fare</th>
      <td>1</td>
      <td>0.113507</td>
    </tr>
    <tr>
      <th>Embarked</th>
      <td>0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Ticket</th>
      <td>0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Parch</th>
      <td>0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>SibSp</th>
      <td>0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Sex</th>
      <td>0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Name</th>
      <td>0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Pclass</th>
      <td>0</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>



**Comment**:  
Based on the above table, the following observations can be made:
1. The cabin feature is also mostly empty, this will be hard to use.
2. The age feature contains a large number of missing values. This will require a smarter approach rather than just filling the null with a median.
3. The fare feature only has 1 missing values. We can come up with an estimate for this by taking a quick look at the data and using the most probable values as a replacement.

<a id="Section_31"></a>
### 3.1 Pivoting features


```python
train[['Sex','Survived']].groupby(['Sex'],as_index=False).agg(['mean','count'])
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">Survived</th>
    </tr>
    <tr>
      <th></th>
      <th>mean</th>
      <th>count</th>
    </tr>
    <tr>
      <th>Sex</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>female</th>
      <td>0.747573</td>
      <td>309</td>
    </tr>
    <tr>
      <th>male</th>
      <td>0.190559</td>
      <td>572</td>
    </tr>
  </tbody>
</table>
</div>




```python
train[['Pclass','Survived']].groupby(['Pclass'],as_index=False).agg(['mean','count'])
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">Survived</th>
    </tr>
    <tr>
      <th></th>
      <th>mean</th>
      <th>count</th>
    </tr>
    <tr>
      <th>Pclass</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.629108</td>
      <td>213</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.472826</td>
      <td>184</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.245868</td>
      <td>484</td>
    </tr>
  </tbody>
</table>
</div>




```python
train[['SibSp','Survived']].groupby(['SibSp'],as_index=False).agg(['mean','count'])
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">Survived</th>
    </tr>
    <tr>
      <th></th>
      <th>mean</th>
      <th>count</th>
    </tr>
    <tr>
      <th>SibSp</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.345395</td>
      <td>608</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.535885</td>
      <td>209</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.464286</td>
      <td>28</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.153846</td>
      <td>13</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.166667</td>
      <td>18</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.000000</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div>




```python
train[['Parch','Survived']].groupby(['Parch'],as_index=False).agg(['mean','count'])
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">Survived</th>
    </tr>
    <tr>
      <th></th>
      <th>mean</th>
      <th>count</th>
    </tr>
    <tr>
      <th>Parch</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.343658</td>
      <td>678</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.550847</td>
      <td>118</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.542857</td>
      <td>70</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.600000</td>
      <td>5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000000</td>
      <td>4</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.200000</td>
      <td>5</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



In conclusion:

- Gender, Females have a higher change of survival over men (74% vs. 19%)
- Pclass, the survival rate is strongly correlated with the passenger class
- SibSp, Parch, smaller family tends to have a higher survival rate

<a id="Section_32"></a>
### 3.2 Embarked Feature   
The embarked feature has missing values in the training set.


```python
# Distribution of the data
train['Embarked'].value_counts(dropna=False)
```




    S      634
    C      168
    Q       77
    NaN      2
    Name: Embarked, dtype: int64



We now inspect the rest of the records for these two missing values:


```python
train[train['Embarked'].isnull()]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>61</th>
      <td>1</td>
      <td>1</td>
      <td>Icard, Miss. Amelie</td>
      <td>female</td>
      <td>38.0</td>
      <td>0</td>
      <td>0</td>
      <td>113572</td>
      <td>80.0</td>
      <td>B28</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>829</th>
      <td>1</td>
      <td>1</td>
      <td>Stone, Mrs. George Nelson (Martha Evelyn)</td>
      <td>female</td>
      <td>62.0</td>
      <td>0</td>
      <td>0</td>
      <td>113572</td>
      <td>80.0</td>
      <td>B28</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>



Two adult females with a fare of $$80 traveling in first class in the same cabin. We use this information by comparing how the fare is distributed based on the embarked location.


```python
# Plot information
fig, ax = plt.subplots(figsize=(15,8),ncols=2)
ax1 = sns.boxplot(x='Embarked',y='Fare',hue='Pclass',data=train,ax=ax[0])
ax2 = sns.boxplot(x='Embarked',y='Fare',hue='Pclass',data=test,ax=ax[1])

ax1.set_title('Training Set',fontsize=15)
ax2.set_title('Test Set',fontsize=15)

ax1.axhline(y=80, color='r', linestyle='-')
ax2.axhline(y=80, color='r', linestyle='-');
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_41_0.png">
</figure>


**Comment**   
Based on the boxplot above, the embarked location corresponding to a fare of $$80 in first class is defined as "C".


```python
# Fill missing values
train.Embarked.fillna('C',inplace=True)
```

<a id="Section_33"></a>
### 3.3 Fare Feature
The test set has a missing value for the Fare feature.


```python
# Display record corresponding to missing value
test[test.Fare.isnull()]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>152</th>
      <td>3</td>
      <td>Storey, Mr. Thomas</td>
      <td>male</td>
      <td>60.5</td>
      <td>0</td>
      <td>0</td>
      <td>3701</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>



We will use the median Fare of the subset corresponding to Pclass=3, Embarked='S', Sex='male', Age>=21 (for adult).


```python
# Extract median
subset_med = test[(test['Pclass']==3) & (test['Embarked']=='S') & (test['Sex']=='male') & (test['Age']>=21)]['Fare'].median()

# Replace missing value
test['Fare'] = test['Fare'].fillna(subset_med)
```


```python
# plot fare distribution
fig, axes = plt.subplots(1,2,figsize=(12,6))
sns.distplot(train['Fare'], color='r', label="Skewness : %.2f"%(train["Fare"].skew()), ax=axes[0]);
sns.distplot(test['Fare'], color='b', label="Skewness : %.2f"%(test["Fare"].skew()), ax=axes[1]);
axes[0].legend()
axes[1].legend();
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_48_0.png">
</figure>


The skewness of the training and test sets corresponding to the Fare feature are very high. In order to help with the prediction, we will apply some normalization using the log of the Fare.


```python
def normalize_fare(train, test):
    train['Fare_log'] = train['Fare'].map(lambda x: np.log(x) if x>0 else 0)
    test['Fare_log'] = test['Fare'].map(lambda x: np.log(x) if x>0 else 0)
    return train, test

train, test = normalize_fare(train, test)
```


```python
train.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Fare_log</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>881.000000</td>
      <td>881.000000</td>
      <td>711.000000</td>
      <td>881.000000</td>
      <td>881.000000</td>
      <td>881.000000</td>
      <td>881.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.385925</td>
      <td>2.307605</td>
      <td>29.731603</td>
      <td>0.455165</td>
      <td>0.363224</td>
      <td>31.121566</td>
      <td>2.874014</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.487090</td>
      <td>0.835055</td>
      <td>14.547835</td>
      <td>0.871571</td>
      <td>0.791839</td>
      <td>47.996249</td>
      <td>0.988903</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.420000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>20.250000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>7.895800</td>
      <td>2.066331</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>28.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>14.454200</td>
      <td>2.670985</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>38.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>30.500000</td>
      <td>3.417727</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>80.000000</td>
      <td>5.000000</td>
      <td>6.000000</td>
      <td>512.329200</td>
      <td>6.238967</td>
    </tr>
  </tbody>
</table>
</div>




```python
# plot fare distribution
fig, axes = plt.subplots(1,2,figsize=(12,6))
sns.distplot(train['Fare_log'], color='r', label="Skewness : %.2f"%(train["Fare_log"].skew()), ax=axes[0]);
sns.distplot(test['Fare_log'], color='b', label="Skewness : %.2f"%(test["Fare_log"].skew()), ax=axes[1]);
axes[0].legend()
axes[1].legend();
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_52_0.png">
</figure>


The skewness of the data has been reduced for both the train and test sets.

<a id="Section_34"></a>
### 3.4 Cabin Feature

The cabin feature is missing for 77% of the training set and 78% of the test set. With such a high percentage, the feature can either be dropped or feature engineering can be used to understand how the cabin id is defined. We opt for the second option.


```python
# Feature inspection
train['Cabin'].sort_values().head(5)
```




    583    A10
    475    A14
    556    A16
    284    A19
    599    A20
    Name: Cabin, dtype: object



It seems that the cabin is made of a letter followed by two digits. After some reading, it appears that the letter corresponds to the cabin deck.  


```python
# Fill missing values with Z00 (it does not correspond to a real deck)
train['Cabin'].fillna('Z00',inplace=True)
test['Cabin'].fillna('Z00',inplace=True)

# Extract deck letter
train['cabin_lett'] = train['Cabin'].apply(lambda x: x[0])
test['cabin_lett'] = test['Cabin'].apply(lambda x: x[0])
```


```python
# Inspect deck distribution based on fare and class
train[['cabin_lett','Fare','Pclass']].groupby(['cabin_lett','Pclass']).median()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>Fare</th>
    </tr>
    <tr>
      <th>cabin_lett</th>
      <th>Pclass</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>A</th>
      <th>1</th>
      <td>35.50000</td>
    </tr>
    <tr>
      <th>B</th>
      <th>1</th>
      <td>80.00000</td>
    </tr>
    <tr>
      <th>C</th>
      <th>1</th>
      <td>83.31665</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">D</th>
      <th>1</th>
      <td>75.25000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>13.00000</td>
    </tr>
    <tr>
      <th rowspan="3" valign="top">E</th>
      <th>1</th>
      <td>55.00000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>11.42500</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12.47500</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">F</th>
      <th>2</th>
      <td>26.00000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.65000</td>
    </tr>
    <tr>
      <th>G</th>
      <th>3</th>
      <td>13.58125</td>
    </tr>
    <tr>
      <th>T</th>
      <th>1</th>
      <td>35.50000</td>
    </tr>
    <tr>
      <th rowspan="3" valign="top">Z</th>
      <th>1</th>
      <td>44.75000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>15.02290</td>
    </tr>
    <tr>
      <th>3</th>
      <td>8.05000</td>
    </tr>
  </tbody>
</table>
</div>



**Comment**   
From the above table, we can see that certain decks were used only by a single class:
- Decks A, B, C, and T are used only by the Upper class
- All three classes share deck E
- Deck D is shared between the upper and middle class
- Deck F is shared between the middle and lower class
- Deck G is used by the lower class only


```python
# Plot information
fig, ax = plt.subplots(figsize=(15,8),ncols=2)
ax1 = sns.boxplot(x='cabin_lett',y='Fare',hue='Pclass',data=train,ax=ax[0])
ax2 = sns.boxplot(x='cabin_lett',y='Fare',hue='Pclass',data=test,ax=ax[1])
ax1.set_title('Training Set',fontsize=15)
ax2.set_title('Test Set',fontsize=15);
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_61_0.png">
</figure>


<a id="Section_35"></a>
### 3.5 Age Feature   

Roughly 20% of the Age data is missing is both the training and test sets. Replacing the missing values by a value such as a median would be oversimplified. We will later implement a K-Nearest Neighbors (KNN) model to to fill the missing values.

*****
<a id="Section_4"></a>
## 4. Data Visualization and Feature Exploration

Before we implement the full model, it is important to inspect the data and answers a few basic questions. This will help understanding how the data is distributed but also will provide useful input used in our models.

Based on the famous rule *"Women and children first"*, we expect the gender and age to be strongly correlated with the survival rate.

Questions:

1. Gender: Is the survival rate higher for females?
2. Age: Is the survival rate higher for young passengers?
3. Pclass & Fare: Is the survival rate higher amongst wealthy passengers?
4. SibSp & Parch: Is the survival rate for family is higher than the one for single passenger?

### 4.1 Gender


```python
# Plot the box plot
pal = {'female':"salmon",'male':"skyblue"}
plt.subplots(figsize = (6,6))
ax = sns.barplot(x = "Sex", y = "Survived", data=train, palette = pal)
plt.title("Impact of gender on the survival rate",fontsize=15)
plt.ylabel("Survival rate",fontsize=15)
plt.xlabel("Sex",fontsize=15);
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_65_0.png">
</figure>



```python
print("Male mean survival rate = \t {:.2f}%".format(train[train.Sex =='male']['Survived'].mean()*100))
print("Female mean survival rate = \t {:.2f}%".format(train[train.Sex != 'male']['Survived'].mean()*100))
```

    Male mean survival rate = 	 19.06%
    Female mean survival rate = 	 74.76%


**Comment**   
Based on the boxplot, the gender appears to be a critical feature when it comes to determining the faith of a passenger. Indeed, females seem to have on average a much higher survival rate.

<a id="Section_42"></a>
### 4.2 Age


```python
# Plot kernel density plot
fig, ax = plt.subplots(figsize=(12,6))
ax = sns.kdeplot(train.loc[(train.Survived==1),'Age'],shade=True,color='blue',label='Survived');
ax = sns.kdeplot(train.loc[(train.Survived==0),'Age'],shade=True,color='red',label='Not Survived')
ax.set_ylabel('Frequency',fontsize=15)
ax.set_xlabel('Age',fontsize=15);
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_69_0.png">
</figure>



```python
print("Children survival rate = \t{:.2f}%".format(train[train.Age <= 12]['Survived'].mean()*100,2))
print("Teenager survival rate = \t{:.2f}%".format(train[(train.Age > 12) & (train.Age <= 20)]['Survived'].mean()*100,2))
print("Adult survival rate = \t\t{:.2f}%".format(train[(train.Age > 20) & (train.Age <= 50)]['Survived'].mean()*100,2))
print("pre-senior survival rate = \t{:.2f}%".format(train[(train.Age > 50) & (train.Age <= 70)]['Survived'].mean()*100,2))
print("senior survival rate = \t\t{:.2f}%".format(train[train.Age > 70]['Survived'].mean()*100,2))
```

    Children survival rate = 	57.97%
    Teenager survival rate = 	38.53%
    Adult survival rate = 		39.23%
    pre-senior survival rate = 	35.59%
    senior survival rate = 		20.00%


**Comment**   
Based on the distribution plot, the age also appears to be a critical feature. Young children have a much higher survival rate on average than the rest of the passenger. The survival rate tends to decrease with the age.

<a id="Section_43"></a>
### 4.3 Pclass and Fare


```python
# Plot the survival rate per class
pal = {1:"gold",2:"silver",3:'sandybrown'}
plt.subplots(figsize = (8,6))
ax = sns.barplot(x = "Pclass", y = "Survived", data=train, palette = pal)
plt.title("Impact of class on the survival rate",fontsize=15)
plt.ylabel("Survival rate",fontsize=15)
plt.xlabel("Class",fontsize=15);
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_73_0.png">
</figure>



```python
print("Upper class survival rate =\t{:.2f}%".format(train[train.Pclass == 1]['Survived'].mean()*100))
print("Middle class survival rate =\t{:.2f}%".format(train[train.Pclass == 2]['Survived'].mean()*100))
print("Lower class survival rate =\t{:.2f}%".format(train[train.Pclass == 3]['Survived'].mean()*100))
```

    Upper class survival rate =	62.91%
    Middle class survival rate =	47.28%
    Lower class survival rate =	24.59%


**Comment**   
The above plot confirms our assumption: upper class passengers had a much higher survival rate.   
   
Before we look at the impact of the fare on the survival rate, we need to verify how the class is correlated to the fare.


```python
# Plot the survival rate per class
pal = {1:"gold",2:"silver",3:'sandybrown'}
plt.subplots(figsize = (8,6))
ax = sns.barplot(x = "Pclass", y = "Fare", data=train, palette = pal)
plt.title("Impact of the passenger class on the fare",fontsize=15)
plt.ylabel("Fare",fontsize=15)
plt.xlabel("Pclass",fontsize=15);
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_76_0.png">
</figure>



```python
print('Median fare:')
print("1st class =\t{:.2f}%".format(train[train.Pclass == 1]['Fare'].median()))
print("2nd class =\t{:.2f}%".format(train[train.Pclass == 2]['Fare'].median()))
print("3nd class =\t{:.2f}%".format(train[train.Pclass == 3]['Fare'].median()))
```

    Median fare:
    1st class =	57.98%
    2nd class =	14.25%
    3nd class =	8.05%



```python
# Box plot
fig, ax = plt.subplots(figsize=(8,6))
sns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = train, ax = ax)
ax.set_title('Pclass vs Fare Survival Comparison',fontsize=15);
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_78_0.png">
</figure>



```python
fig = plt.figure(figsize=(15,6),)
ax=sns.distplot(train['Fare'] , color='blue',kde=False,bins=20)
ax.set_ylabel("Count",fontsize=15)
ax.set_title('Fare distribution',fontsize=15)
ax.set_xticks(range(0,600,25));
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_79_0.png">
</figure>



```python
# Kernel Density Plot
fig = plt.figure(figsize=(15,8),)
ax=sns.distplot(train.loc[(train['Survived'] == 0),'Fare'] , color='red',label='Not Survived',kde=False,bins=21)
ax=sns.distplot(train.loc[(train['Survived'] == 1),'Fare'] , color='blue', label='Survived',kde=False,bins=41)
ax.legend(['Not Survived','Survived'])
plt.xlabel("Fare",fontsize=15)
plt.ylabel("Frequency of Passenger Survived",fontsize=15)
plt.title('Fare Distribution Survived vs Non Survived',fontsize=15);
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_80_0.png">
</figure>


**Comment**   
As expected. the survival rate increases with the fare price. Based on the above plot, it appears that the survival rate is larger than 50% for fares higher that $$150.

<a id="Section_44"></a>
### 4.4 SibSp & Parch


```python
train[['SibSp','Survived']].groupby(['SibSp'],as_index=False).mean().sort_values(by='Survived',ascending =False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>SibSp</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0.535885</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>0.464286</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0.345395</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>0.166667</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>0.153846</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
sns.barplot(x="SibSp", y="Survived",color='r',data=train);
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_84_0.png">
</figure>



```python
train[['Parch','Survived']].groupby(['Parch'],as_index=False).mean().sort_values(by='Survived',ascending =False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Parch</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>0.600000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0.550847</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>0.542857</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0.343658</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5</td>
      <td>0.200000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>6</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
sns.barplot(x="Parch", y="Survived",color='b', data=train);
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_86_0.png">
</figure>


**Comment**   
Passenger traveling with large family decreased the survival rate.

<a id="Section_45"></a>
### 4.5 Embarked


```python
train[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean().sort_values(by='Survived',ascending =False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Embarked</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>C</td>
      <td>0.558824</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Q</td>
      <td>0.389610</td>
    </tr>
    <tr>
      <th>2</th>
      <td>S</td>
      <td>0.339117</td>
    </tr>
  </tbody>
</table>
</div>



*****
<a id="Section_5"></a>
## 5. Statistical Study

In this section, we will inspect the data and quantify the observations that result from the data visualization.

<a id="Section_51"></a>
### 5.1 Main Features


```python
# Turning the Sex feature into a boolean classifier
train['Sex'] = train['Sex'].apply(lambda x: 0 if x == "female" else 1)
test['Sex'] = test['Sex'].apply(lambda x: 0 if x == "female" else 1)
```


```python
train.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Fare_log</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>881.000000</td>
      <td>881.000000</td>
      <td>881.000000</td>
      <td>711.000000</td>
      <td>881.000000</td>
      <td>881.000000</td>
      <td>881.000000</td>
      <td>881.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.385925</td>
      <td>2.307605</td>
      <td>0.649262</td>
      <td>29.731603</td>
      <td>0.455165</td>
      <td>0.363224</td>
      <td>31.121566</td>
      <td>2.874014</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.487090</td>
      <td>0.835055</td>
      <td>0.477472</td>
      <td>14.547835</td>
      <td>0.871571</td>
      <td>0.791839</td>
      <td>47.996249</td>
      <td>0.988903</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.420000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>20.250000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>7.895800</td>
      <td>2.066331</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>28.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>14.454200</td>
      <td>2.670985</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>38.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>30.500000</td>
      <td>3.417727</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>80.000000</td>
      <td>5.000000</td>
      <td>6.000000</td>
      <td>512.329200</td>
      <td>6.238967</td>
    </tr>
  </tbody>
</table>
</div>



**Comment**   
From the statistical data above, it appears that only 38% of the passengers survived.


```python
train[['Sex', 'Survived']].groupby("Sex").mean()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
    </tr>
    <tr>
      <th>Sex</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.747573</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.190559</td>
    </tr>
  </tbody>
</table>
</div>




```python
train[['Pclass', 'Survived']].groupby("Pclass").mean()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
    </tr>
    <tr>
      <th>Pclass</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.629108</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.472826</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.245868</td>
    </tr>
  </tbody>
</table>
</div>



<a id="Section_52"></a>
### 5.2 Correlation Study


```python
# Feature correlation
train.corr()['Survived'].sort_values()
```




    Sex        -0.546015
    Pclass     -0.334097
    Age        -0.076867
    SibSp       0.003330
    Parch       0.092819
    Fare        0.264613
    Fare_log    0.343248
    Survived    1.000000
    Name: Survived, dtype: float64




```python
# Compute the correlation matrix
corr = train.corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(10, 12))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0,vmin=-1.0, center=0,annot=True,
            square=True, linewidths=.5, cbar_kws={"shrink": .5});
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_100_0.png">
</figure>


**Comment**   

Strong positive correlations:
- Parch and SibSp (0.41)
- Fare and Survived (0.26)
- Parch and Fare (0.22)
   
Strong negative correlation
- Fare and Pclass (-0.55)
- Sex and Survived (-0.54)
- Pclass and Age (-0.37)
- Pclass and Survived (-0.34)

*******
<a id="Section_6"></a>
## 6. Feature Preparation

Based on the knowledge gathered, we can now create new features that will help improve the model accuracy.

<a id="Section_1"></a>
### 6.1 Name

#### 6.1.1 Passenger Title
Upon inspection of the Name feature, it appear that a title is assigned to each passenger. We extract this feature and store it in the dataset.


```python
# extract new feature using regular expression
train['name_title'] = train['Name'].str.extract(r' ([A-Za-z]+)\.',expand=False)
test['name_title'] = test['Name'].str.extract(r' ([A-Za-z]+)\.',expand=False)
```


```python
train['name_title'].value_counts()
```




    Mr          513
    Miss        177
    Mrs         125
    Master       39
    Dr            7
    Rev           6
    Col           2
    Mlle          2
    Major         2
    Ms            1
    Mme           1
    Sir           1
    Countess      1
    Lady          1
    Don           1
    Capt          1
    Jonkheer      1
    Name: name_title, dtype: int64




```python
# Investigate survival based on title
train['Survived'].groupby(train['name_title']).mean()
```




    name_title
    Capt        0.000000
    Col         0.500000
    Countess    1.000000
    Don         0.000000
    Dr          0.428571
    Jonkheer    0.000000
    Lady        1.000000
    Major       0.500000
    Master      0.589744
    Miss        0.706215
    Mlle        1.000000
    Mme         1.000000
    Mr          0.157895
    Mrs         0.792000
    Ms          1.000000
    Rev         0.000000
    Sir         1.000000
    Name: Survived, dtype: float64



As shown above, several title possess a high survival rate. We will account for it in our model. We will now investigate the correlation between the passenger's name and his/her survival rate.

**Comment**  
Based on the results shown above, it appears that different title are used to describe the same status. For instance Miss, Mlle, and Ms are used to describe Miss. We standardize the titles using a custom function.

#### 6.1.2 Name length  
Some additional inspection seems to indicate that the more complex the name of a passenger the higher the survival rate. Therefore, we include this idea in our model.


```python
# Determine if correlation can be found between name length and survival rate
train['name_len'] = train['Name'].apply(lambda x: len(x))
train['Survived'].groupby(pd.qcut(train['name_len'],6)).mean()
```




    name_len
    (11.999, 19.0]    0.221675
    (19.0, 22.0]      0.307692
    (22.0, 25.0]      0.335821
    (25.0, 28.0]      0.316547
    (28.0, 33.333]    0.482270
    (33.333, 82.0]    0.693878
    Name: Survived, dtype: float64




```python
pd.qcut(train['name_len'],6).value_counts()
```




    (11.999, 19.0]    203
    (33.333, 82.0]    147
    (28.0, 33.333]    141
    (25.0, 28.0]      139
    (22.0, 25.0]      134
    (19.0, 22.0]      117
    Name: name_len, dtype: int64




```python
# compute mean survival rate based on name length
mean_survival_name_length = train[['name_len','Survived']].groupby('name_len').mean()
```


```python
fig = plt.figure(figsize=(16,6),)
ax=sns.distplot(train['name_len'] , color='R',kde=False,bins=20)
ax.set_ylabel("Count",fontsize=15)
ax.set_title('Name length distribution and Survival rate',fontsize=15)
ax.set_ylim(0,220)
ax.set_xlim(10,90)
ax.set_yticks([0.0,50,100,150,200])

ax2 = ax.twinx()
ax2.set_ylim(0,1.1)
ax2.plot(mean_survival_name_length)
ax2.set_yticks([0.0,0.25,0.5,0.75,1.0])
ax2.set_ylabel("Survival Rate",fontsize=15)
fig.tight_layout();
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_114_0.png">
</figure>


It appears that the survival rate is positively correlated to the passenger's name length.
The next feature that we will investigate is the ticket ID. Even if the available data does not help locating the passenger on the boat, a pattern in the ticket ID can help us.

<a id="Section_62"></a>
### 6.2 Ticket


```python
train['Ticket'].head()
```




    0           A/5 21171
    1            PC 17599
    2    STON/O2. 3101282
    3              113803
    4              373450
    Name: Ticket, dtype: object




```python
train['Ticket_Len'] = train['Ticket'].apply(lambda x: len(x))
train['Ticket_Len'].value_counts()
```




    6     419
    5     128
    4     101
    8      69
    10     41
    7      27
    9      26
    17     14
    16     11
    13     10
    12     10
    15      9
    11      8
    18      6
    3       2
    Name: Ticket_Len, dtype: int64



One specific aspect of the ticket ID appears to be helpful. A large portion of the tickets seem to start with a letter, other are simply made of digits.


```python
train['ticket_lett'] = train['Ticket'].apply(lambda x: str(x)[0])
```


```python
train[['ticket_lett','Survived']].groupby(['ticket_lett'],as_index=False).agg(['mean','count'])
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">Survived</th>
    </tr>
    <tr>
      <th></th>
      <th>mean</th>
      <th>count</th>
    </tr>
    <tr>
      <th>ticket_lett</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.629371</td>
      <td>143</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.464481</td>
      <td>183</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.239203</td>
      <td>301</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.200000</td>
      <td>10</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.000000</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.166667</td>
      <td>6</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.111111</td>
      <td>9</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.000000</td>
      <td>2</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>A</th>
      <td>0.068966</td>
      <td>29</td>
    </tr>
    <tr>
      <th>C</th>
      <td>0.400000</td>
      <td>40</td>
    </tr>
    <tr>
      <th>F</th>
      <td>0.571429</td>
      <td>7</td>
    </tr>
    <tr>
      <th>L</th>
      <td>0.250000</td>
      <td>4</td>
    </tr>
    <tr>
      <th>P</th>
      <td>0.646154</td>
      <td>65</td>
    </tr>
    <tr>
      <th>S</th>
      <td>0.323077</td>
      <td>65</td>
    </tr>
    <tr>
      <th>W</th>
      <td>0.153846</td>
      <td>13</td>
    </tr>
  </tbody>
</table>
</div>



In conclusion, the ticket fare can be integrated in our model by using the first character of the ticket ID. The fare is now investigated.

<a id="Section_63"></a>
### 6.3 Fare


```python
train['Survived'].groupby(pd.qcut(train['Fare'], 5)).agg(['mean','count'])
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>count</th>
    </tr>
    <tr>
      <th>Fare</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(-0.001, 7.854]</th>
      <td>0.217877</td>
      <td>179</td>
    </tr>
    <tr>
      <th>(7.854, 10.5]</th>
      <td>0.201087</td>
      <td>184</td>
    </tr>
    <tr>
      <th>(10.5, 21.0]</th>
      <td>0.437126</td>
      <td>167</td>
    </tr>
    <tr>
      <th>(21.0, 39.0]</th>
      <td>0.443182</td>
      <td>176</td>
    </tr>
    <tr>
      <th>(39.0, 512.329]</th>
      <td>0.645714</td>
      <td>175</td>
    </tr>
  </tbody>
</table>
</div>




```python
pd.crosstab(pd.qcut(train['Fare'], 5), columns=train['Pclass'])
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Pclass</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
    </tr>
    <tr>
      <th>Fare</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(-0.001, 7.854]</th>
      <td>6</td>
      <td>6</td>
      <td>167</td>
    </tr>
    <tr>
      <th>(7.854, 10.5]</th>
      <td>0</td>
      <td>24</td>
      <td>160</td>
    </tr>
    <tr>
      <th>(10.5, 21.0]</th>
      <td>0</td>
      <td>80</td>
      <td>87</td>
    </tr>
    <tr>
      <th>(21.0, 39.0]</th>
      <td>61</td>
      <td>64</td>
      <td>51</td>
    </tr>
    <tr>
      <th>(39.0, 512.329]</th>
      <td>146</td>
      <td>10</td>
      <td>19</td>
    </tr>
  </tbody>
</table>
</div>



The survival rate is positively correlated with the fare. Moreover, as expected, the class is also correlated with the fare. We will now investigate the embarkation feature.

<a id="Section_64"></a>
### 6.4 Cabin


```python
train[train.Cabin.notnull()][['Survived','Cabin']].head(5)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Cabin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>Z00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>C85</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>Z00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>C123</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>Z00</td>
    </tr>
  </tbody>
</table>
</div>



The cabin ID is made of a letter character and a combination of digits. We create a new feature based on the first letter (which might be the deck number).


```python
train['cabin_lett'] = train['Cabin'].apply(lambda x: str(x)[0])
```


```python
train['Survived'].groupby(train['cabin_lett']).agg(['mean','count'])
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>count</th>
    </tr>
    <tr>
      <th>cabin_lett</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>A</th>
      <td>0.466667</td>
      <td>15</td>
    </tr>
    <tr>
      <th>B</th>
      <td>0.744681</td>
      <td>47</td>
    </tr>
    <tr>
      <th>C</th>
      <td>0.589286</td>
      <td>56</td>
    </tr>
    <tr>
      <th>D</th>
      <td>0.757576</td>
      <td>33</td>
    </tr>
    <tr>
      <th>E</th>
      <td>0.750000</td>
      <td>32</td>
    </tr>
    <tr>
      <th>F</th>
      <td>0.615385</td>
      <td>13</td>
    </tr>
    <tr>
      <th>G</th>
      <td>0.500000</td>
      <td>4</td>
    </tr>
    <tr>
      <th>T</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Z</th>
      <td>0.302941</td>
      <td>680</td>
    </tr>
  </tbody>
</table>
</div>



A correlation exists between the first letter and the survival rate. We will include this feature in our model. We will now investigate the cabin number.


```python
# Several record contains multiple cabin number, we will focus on the first one.
train['cabin_num'] = train['Cabin'].apply(lambda x: str(x).split(' ')[0][1:])
train['cabin_num'].unique()
```




    array(['00', '85', '123', '46', '6', '103', '56', '78', '33', '30', '52',
           '28', '83', '', '31', '5', '10', '26', '110', '58', '101', '47',
           '86', '2', '19', '7', '49', '4', '32', '80', '36', '15', '93',
           '35', '87', '77', '67', '94', '125', '99', '118', '22', '106',
           '65', '54', '57', '34', '18', '124', '91', '40', '128', '37', '50',
           '82', '96', '44', '23', '104', '111', '92', '38', '21', '12', '63',
           '14', '20', '79', '25', '73', '95', '39', '70', '16', '68', '41',
           '9', '48', '126', '71', '51', '62', '24', '90', '45', '8', '121',
           '11', '3', '17', '102', '69', '42', '148'], dtype=object)




```python
train.groupby(['cabin_lett','cabin_num'])['Survived'].agg(['mean','count'])
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>mean</th>
      <th>count</th>
    </tr>
    <tr>
      <th>cabin_lett</th>
      <th>cabin_num</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="15" valign="top">A</th>
      <th>10</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>16</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>23</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>26</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>31</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>34</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th rowspan="15" valign="top">B</th>
      <th>101</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>102</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>18</th>
      <td>1.000000</td>
      <td>2</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1.000000</td>
      <td>2</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.500000</td>
      <td>2</td>
    </tr>
    <tr>
      <th>28</th>
      <td>1.000000</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>35</th>
      <td>1.000000</td>
      <td>2</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>39</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>41</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th rowspan="22" valign="top">E</th>
      <th>101</th>
      <td>1.000000</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>121</th>
      <td>1.000000</td>
      <td>2</td>
    </tr>
    <tr>
      <th>17</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>24</th>
      <td>1.000000</td>
      <td>2</td>
    </tr>
    <tr>
      <th>25</th>
      <td>1.000000</td>
      <td>2</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>33</th>
      <td>1.000000</td>
      <td>2</td>
    </tr>
    <tr>
      <th>34</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>36</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>40</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>44</th>
      <td>0.500000</td>
      <td>2</td>
    </tr>
    <tr>
      <th>46</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>49</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>50</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>58</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>63</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>67</th>
      <td>0.500000</td>
      <td>2</td>
    </tr>
    <tr>
      <th>68</th>
      <td>1.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>77</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1.000000</td>
      <td>2</td>
    </tr>
    <tr>
      <th rowspan="5" valign="top">F</th>
      <th></th>
      <td>0.250000</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.666667</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>1.000000</td>
      <td>3</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.000000</td>
      <td>2</td>
    </tr>
    <tr>
      <th>G</th>
      <th>6</th>
      <td>0.500000</td>
      <td>4</td>
    </tr>
    <tr>
      <th>T</th>
      <th></th>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Z</th>
      <th>00</th>
      <td>0.302941</td>
      <td>680</td>
    </tr>
  </tbody>
</table>
<p>146 rows  2 columns</p>
</div>



****
<a id="Section_7"></a>
## 7. Cleaning Functions


```python
def names(train, test):
    '''
    Create the name_len and name_title feature
    '''
    for i in [train, test]:
        
        i['name_len'] = i['Name'].apply(lambda x: len(x))
        i['name_title'] = i['Name'].str.extract(r' ([A-Za-z]+)\.',expand=False)
        
        # bin titles
        i['name_title'] = i['name_title'].replace(['Lady', 'the Countess','Countess','Capt',
                                                           'Col','Don', 'Dr', 'Major', 'Rev', 'Sir',
                                                           'Jonkheer', 'Dona'], 'Rare')

        i['name_title'] = i['name_title'].replace(['Miss', 'Mrs', 'Mlle', 'Mme'], 'Ms')
        
        del i['Name']
        
    return train, test
```


```python
def family_size(train, test):
    '''
    Create a family size feature which is defined as the sum of the SibSp and Parch features
    '''
    for i in [train, test]:
        i['fam_size_gr'] = np.where((i['SibSp']+i['Parch']) == 0 , 'Solo',
                           np.where((i['SibSp']+i['Parch']) <= 3,'Small', 'Large'))
        
        
    return train, test
```


```python
def ticket_features(train, test):
    '''
    Create the ticket_lett feature and group it for the low survival rate.
    '''
    for i in [train, test]:
        i['ticket_lett'] = i['Ticket'].apply(lambda x: str(x)[0])
        
        i['ticket_lett'] = i['ticket_lett'].apply(lambda x: str(x))
        
        i['ticket_lett'] = np.where((i['ticket_lett']).isin(['1', '2', 'F','S', 'P', 'C','9']), i['ticket_lett'],
                                   np.where((i['ticket_lett']).isin(['A','W','3','4', '7', '6', 'L', '5', '8']),
                                            'low_ticket', 'other_ticket'))
        
        i['Ticket_Len'] = i['Ticket'].apply(lambda x: len(x))
        
        del i['Ticket']
        
    return train, test
```


```python
def cabin_letter(train, test):
    '''
    Create the cabin_lett feature.
    '''
    for i in [train, test]:
        i['cabin_lett'] = i['Cabin'].apply(lambda x: str(x)[0])
        
        del i['Cabin']
        
    return train, test
```


```python
def cabin_num(train, test):
    '''
    Convert the cabin number using the quantile group and get_dummies sub.
    '''
    for i in [train, test]:
        # Extract first cabin number
        i['cabin_num_1st'] = i['Cabin'].apply(lambda x: str(x).split(' ')[0][1:])
        # Fill missing value and convert to int when possible
        i['cabin_num_1st'].replace('an', np.NaN, inplace = True)
        i['cabin_num_1st'] = i['cabin_num_1st'].apply(lambda x: int(x) if not pd.isnull(x) and x != '' else np.NaN)
        i['cabin_num'] = pd.qcut(train['cabin_num_1st'],5)
        
    train = pd.concat((train, pd.get_dummies(train['cabin_num'], prefix = 'cabin_num')), axis = 1)
    test = pd.concat((test, pd.get_dummies(test['cabin_num'], prefix = 'cabin_num')), axis = 1)
    
    del train['cabin_num']
    del test['cabin_num']
    del train['cabin_num_1st']
    del test['cabin_num_1st']
    
    return train, test
```


```python
def embarked_station(train, test):
    '''
    Replace missing value with most probable value
    '''
    for i in [train, test]:
        i['Embarked'] = i['Embarked'].fillna('C')
    return train, test
```


```python
def fare(train, test):
    '''
    Replace missing value with most probable value
    '''
    # Extract median
    subset_med = test[(test['Pclass']==3) & (test['Embarked']=='S') & (test['Sex']=='male') & (test['Age']>=21)]['Fare'].median()

    # Replace missing value
    test['Fare'] = test['Fare'].fillna(subset_med)
    
    train , test = normalize_fare(train, test)
    
    # bin results
    train['FareBin'] = pd.qcut(train['Fare'], 4)
    test['FareBin'] = pd.qcut(test['Fare'], 4)

    
    return train, test    
```


```python
def dummies(train, test, columns = ['Pclass', 'Sex', 'Embarked', 'ticket_lett', 'cabin_lett', 'name_title', 'fam_size', 'fam_size_sq']):
    '''
    Convert categorical features into dummies.
    '''
    for column in columns:
        #Convert features in strings
        train[column] = train[column].apply(lambda x: str(x))
        test[column] = test[column].apply(lambda x: str(x))
        
        # Create unique feature name using the unique values of each feature (inner train and test)
        good_cols = [column+'_'+i for i in train[column].unique() if i in test[column].unique()]
        
        # Concat the dummies to the main DF and delete the original columns
        train = pd.concat((train, pd.get_dummies(train[column], prefix = column)[good_cols]), axis = 1)
        test = pd.concat((test, pd.get_dummies(test[column], prefix = column)[good_cols]), axis = 1)
        
        del train[column]
        del test[column]
    return train, test
```


```python
def drop(train, test, to_drop):
    '''
    Drop listed features
    '''
    for i in [train, test]:
        for z in to_drop:
            del i[z]
    return train, test
```


```python
def fill_age_knn(train, test):
    '''
    Fill missing Age values using KNN    
    '''
    # Move the age feature as index to be used with Fancy Input
    front = train['Age']
    train.drop(labels=['Age'], axis=1,inplace = True)
    train.insert(0, 'Age', front)
       
    # Move the age feature as index to be used with Fancy Input
    front = test['Age']
    test.drop(labels=['Age'], axis=1,inplace = True)
    test.insert(0, 'Age', front)
    
    # fill missing values using KNN for age column. 
    age_train = KNN_fi(k=10).fit_transform(train)
    age_test = KNN_fi(k=10).fit_transform(test)
    
    # Recombine the data
    train = pd.DataFrame(age_train, columns = train.columns)
    test = pd.DataFrame(age_test, columns = test.columns)

    return train, test
```


```python
def fill_missing(train, test):
    '''
    Fill missing values
    '''
    train = train.Embarked.fillna('C')

    return train, test
```


```python
# Apply cleaning and feature conversion
train = pd.read_csv('./Data/train.csv')
test = pd.read_csv('./Data/test.csv')

train, test = names(train, test)
train, test = cabin_num(train, test)
train, test = cabin_letter(train, test)
train, test = embarked_station(train, test)
train, test = family_size(train, test)
train, test = fare(train, test)

train, test = ticket_features(train, test)
train, test = dummies(train, test, columns = ['Pclass', 'Sex', 'Embarked', 'ticket_lett',
                                              'cabin_lett', 'name_title', 'fam_size_gr', 'FareBin'])

train, test = drop(train, test, ['PassengerId'])
train = del_outliers(train, detect_outliers(train, ['SibSp', 'Parch', 'Fare'], 2))

train, test = fill_age_knn(train, test)
```

    Imputing row 1/881 with 0 missing, elapsed time: 0.257
    Imputing row 101/881 with 0 missing, elapsed time: 0.258
    Imputing row 201/881 with 0 missing, elapsed time: 0.259
    Imputing row 301/881 with 0 missing, elapsed time: 0.260
    Imputing row 401/881 with 0 missing, elapsed time: 0.261
    Imputing row 501/881 with 1 missing, elapsed time: 0.262
    Imputing row 601/881 with 0 missing, elapsed time: 0.263
    Imputing row 701/881 with 0 missing, elapsed time: 0.264
    Imputing row 801/881 with 0 missing, elapsed time: 0.264
    Imputing row 1/418 with 0 missing, elapsed time: 0.056
    Imputing row 101/418 with 0 missing, elapsed time: 0.065
    Imputing row 201/418 with 1 missing, elapsed time: 0.066
    Imputing row 301/418 with 0 missing, elapsed time: 0.067
    Imputing row 401/418 with 0 missing, elapsed time: 0.068



```python
train = train.rename(columns = {'cabin_num_(1.999, 19.0]':'cabin_num_(1.999_19.0)',
                        'cabin_num_(19.0, 33.0]':'cabin_num_(19.0_33.0)',
                        'cabin_num_(33.0, 50.0]':'cabin_num_(33.0_50.0)',
                        'cabin_num_(50.0, 86.0]':'cabin_num_(50.0_86.0)',
                        'cabin_num_(86.0, 148.0]':'cabin_num_(86.0_148.0)'})
```


```python
test = test.rename(columns = {'cabin_num_(1.999, 19.0]':'cabin_num_(1.999_19.0)',
                        'cabin_num_(19.0, 33.0]':'cabin_num_(19.0_33.0)',
                        'cabin_num_(33.0, 50.0]':'cabin_num_(33.0_50.0)',
                        'cabin_num_(50.0, 86.0]':'cabin_num_(50.0_86.0)',
                        'cabin_num_(86.0, 148.0]':'cabin_num_(86.0_148.0)'})
```

****
<a id="Section_8"></a>
## 8. Model Preparation

We will now prepare the data before creating a model. The preparation is divided into three steps:
1. Separate the dataframe into our input data and our output feature (X and y).
2. Normalize the data


```python
#scaler = StandardScaler()
#scaler.fit(train)
#train = scaler.transform(train)
#test = scaler.transform(test)
```


```python
# Column compatibility
print("Feature discrepancies between train and test:")
print(train.columns.difference(test.columns))
```

    Feature discrepancies between train and test:
    Index(['Survived'], dtype='object')



```python
# Column compatibility
print("Feature discrepancies between train and test:")
print(test.columns.difference(train.columns))
```

    Feature discrepancies between train and test:
    Index([], dtype='object')



```python
# Step 1: define X and y
X_train = train.drop(['Survived'],axis=1)
y_train = train['Survived']
```

****
<a id="Section_9"></a>
## 9. Models

In this section, we will make predictions using the following models:
 - RandomForestClassifier
 - ExtraTreesClassifier
 - LogisticRegression
 - GradientBoostingClassifier
 - LinearDiscriminantAnalysis
 - RidgeClassifier
 - XGBClassifier
 - MLPClassifier
 - BaggingClassifier
 - BernoulliNB
 - ExtraTreeClassifier
 - DecisionTreeClassifier
 - LinearSVC
 - AdaBoostClassifier
 - SVC
 - NuSVC
 - SGDClassifier
 - Perceptron
 - GaussianProcessClassifier
 - KNeighborsClassifier
 - GaussianNB
 - PassiveAggressiveClassifier
 - QuadraticDiscriminantAnalysis


```python
# Cross validate model with Kfold stratified cross val
kfold = StratifiedKFold(n_splits=10)
```


```python
random_state = 42
```


```python
# classifiers
classifiers_list = [
    #Ensemble Methods
    AdaBoostClassifier(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state)),
    BaggingClassifier(random_state=random_state),
    ExtraTreesClassifier(random_state=random_state),
    GradientBoostingClassifier(random_state=random_state),
    RandomForestClassifier(random_state=random_state),

    #Gaussian Processes
    GaussianProcessClassifier(random_state=random_state),
    
    #GLM
    LogisticRegression(random_state=random_state),
    PassiveAggressiveClassifier(random_state=random_state),
    RidgeClassifier(),
    SGDClassifier(random_state=random_state),
    Perceptron(random_state=random_state),
    MLPClassifier(random_state=random_state),
    
    #Navies Bayes
    BernoulliNB(),
    GaussianNB(),
    
    #Nearest Neighbor
    KNeighborsClassifier(),
    
    #SVM
    SVC(probability=True, random_state=random_state),
    NuSVC(probability=True, random_state=random_state),
    LinearSVC(random_state=random_state),
    
    #Trees    
    DecisionTreeClassifier(random_state=random_state),
    ExtraTreeClassifier(random_state=random_state),
    
    #Discriminant Analysis
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis(),

    
    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html
    XGBClassifier()    
    ]

# store cv results in list
cv_results_list = []
cv_means_list = []
cv_std_list = []

# perform cross-validation
for clf in classifiers_list:
    cv_results_list.append(cross_val_score(clf,
                                           X_train,
                                           y_train,
                                           scoring = "accuracy",
                                           cv = kfold,
                                           n_jobs=4))

# store mean and std accuracy
for cv_result in cv_results_list:
    cv_means_list.append(cv_result.mean())
    cv_std_list.append(cv_result.std())
                      
cv_res_df = pd.DataFrame({"CrossValMeans":cv_means_list,
                          "CrossValerrors": cv_std_list,
                          "Algorithm":[clf.__class__.__name__ for clf in classifiers_list]})                    

cv_res_df = cv_res_df.sort_values(by='CrossValMeans',ascending=False)             
```


```python
cv_res_df.set_index('Algorithm')
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CrossValMeans</th>
      <th>CrossValerrors</th>
    </tr>
    <tr>
      <th>Algorithm</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>RandomForestClassifier</th>
      <td>0.839990</td>
      <td>0.039663</td>
    </tr>
    <tr>
      <th>ExtraTreesClassifier</th>
      <td>0.833159</td>
      <td>0.035874</td>
    </tr>
    <tr>
      <th>LogisticRegression</th>
      <td>0.833159</td>
      <td>0.035874</td>
    </tr>
    <tr>
      <th>GradientBoostingClassifier</th>
      <td>0.833146</td>
      <td>0.040341</td>
    </tr>
    <tr>
      <th>LinearDiscriminantAnalysis</th>
      <td>0.832035</td>
      <td>0.039495</td>
    </tr>
    <tr>
      <th>RidgeClassifier</th>
      <td>0.832022</td>
      <td>0.039233</td>
    </tr>
    <tr>
      <th>XGBClassifier</th>
      <td>0.832022</td>
      <td>0.026210</td>
    </tr>
    <tr>
      <th>MLPClassifier</th>
      <td>0.828613</td>
      <td>0.036400</td>
    </tr>
    <tr>
      <th>BaggingClassifier</th>
      <td>0.825255</td>
      <td>0.036360</td>
    </tr>
    <tr>
      <th>BernoulliNB</th>
      <td>0.795684</td>
      <td>0.035216</td>
    </tr>
    <tr>
      <th>ExtraTreeClassifier</th>
      <td>0.795684</td>
      <td>0.047406</td>
    </tr>
    <tr>
      <th>DecisionTreeClassifier</th>
      <td>0.790028</td>
      <td>0.045720</td>
    </tr>
    <tr>
      <th>LinearSVC</th>
      <td>0.785470</td>
      <td>0.066153</td>
    </tr>
    <tr>
      <th>AdaBoostClassifier</th>
      <td>0.782074</td>
      <td>0.055121</td>
    </tr>
    <tr>
      <th>SVC</th>
      <td>0.765066</td>
      <td>0.036109</td>
    </tr>
    <tr>
      <th>NuSVC</th>
      <td>0.765054</td>
      <td>0.032043</td>
    </tr>
    <tr>
      <th>SGDClassifier</th>
      <td>0.738917</td>
      <td>0.091090</td>
    </tr>
    <tr>
      <th>Perceptron</th>
      <td>0.737819</td>
      <td>0.106109</td>
    </tr>
    <tr>
      <th>GaussianProcessClassifier</th>
      <td>0.735598</td>
      <td>0.044374</td>
    </tr>
    <tr>
      <th>KNeighborsClassifier</th>
      <td>0.732163</td>
      <td>0.040722</td>
    </tr>
    <tr>
      <th>GaussianNB</th>
      <td>0.728754</td>
      <td>0.049676</td>
    </tr>
    <tr>
      <th>PassiveAggressiveClassifier</th>
      <td>0.686593</td>
      <td>0.111011</td>
    </tr>
    <tr>
      <th>QuadraticDiscriminantAnalysis</th>
      <td>0.666305</td>
      <td>0.036540</td>
    </tr>
  </tbody>
</table>
</div>




```python
# plot results
fig, ax = plt.subplots(figsize=(12,12))
g = sns.barplot("CrossValMeans",
                "Algorithm",
                data = cv_res_df,
                palette="Set3",
                orient = "h",
                **{'xerr':cv_std_list})
g.set_xlabel("Mean Accuracy")
g = g.set_title("Cross validation scores")
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_161_0.png">
</figure>


Based on the previous observations, the baseline accuracy is 67.5%

The following models are chosen for the ensemble modeling (iteration process):
1. Extra Trees
2. SVC
3. RandomForest
4. LDA
5. Logistic Regression


```python
# Extra Trees
ExtraTrees = ExtraTreesClassifier(random_state=random_state)

extraTrees_param_grid = {"max_depth": [None],
              "max_features": [1, 3, 10],
              "min_samples_split": [2, 3, 10],
              "min_samples_leaf": [1, 3, 10],
              "bootstrap": [False],
              "n_estimators" :[100,300],
              "criterion": ["gini"]}
                                 
gs_extraTrees = GridSearchCV(ExtraTrees,
                        param_grid = extraTrees_param_grid,
                        cv=kfold,
                        scoring="accuracy",
                        n_jobs=4,
                        verbose = 1)

gs_extraTrees.fit(X_train,y_train)

extraTrees_best = gs_extraTrees.best_estimator_

gs_extraTrees.best_score_
```
    Fitting 10 folds for each of 54 candidates, totalling 540 fits

    [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.
    [Parallel(n_jobs=4)]: Done  76 tasks      | elapsed:    5.9s
    [Parallel(n_jobs=4)]: Done 376 tasks      | elapsed:   26.3s
    [Parallel(n_jobs=4)]: Done 540 out of 540 | elapsed:   39.7s finished

    0.8433598183881952
    
```python
# SVC
SVmC = SVC(probability=True, random_state=random_state)

svc_param_grid = {'kernel': ['rbf'], 
                  'gamma': [ 0.001, 0.01, 0.1, 1],
                  'C': [1, 10, 50, 100,200,300, 1000]}

gs_svc = GridSearchCV(SVmC,
                        param_grid = svc_param_grid,
                        cv=kfold,
                        scoring="accuracy",
                        n_jobs=4,
                        verbose = 1)

gs_svc.fit(X_train,y_train)

svc_best = gs_svc.best_estimator_

gs_svc.best_score_
```
    [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.
    
    Fitting 10 folds for each of 28 candidates, totalling 280 fits
    
    [Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.9s
    [Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   16.1s
    [Parallel(n_jobs=4)]: Done 280 out of 280 | elapsed:   30.1s finished
    
    0.8104426787741204
    
```python
# Random Forest Classifier
RdmForest = RandomForestClassifier(random_state=random_state)

rdf_param_grid = {"max_depth": [None],
                  "max_features": [1, 3, 10],
                  "min_samples_split": [2, 3, 10],
                  "min_samples_leaf": [1, 3, 10],
                  "bootstrap": [False],
                  "n_estimators" :[100,300],
                  "criterion": ["gini"]}

gs_rdf = GridSearchCV(RdmForest,
                        param_grid = rdf_param_grid,
                        cv=kfold,
                        scoring="accuracy",
                        n_jobs=4,
                        verbose = 1)

gs_rdf.fit(X_train,y_train)

rdf_best = gs_rdf.best_estimator_

gs_rdf.best_score_
```
    Fitting 10 folds for each of 54 candidates, totalling 540 fits

    [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.
    [Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    5.2s
    [Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   17.5s
    [Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:   42.2s
    [Parallel(n_jobs=4)]: Done 540 out of 540 | elapsed:   58.1s finished

    0.8456299659477866
    
```python
# LDA
LDA = LinearDiscriminantAnalysis()

LDA.fit(X_train,y_train)

LDA_best = LDA
```


```python
# Logistic Regression
LR = LogisticRegression(random_state=random_state)

lr_param_grid = {"C":np.logspace(-3,3,10), "penalty":["l1","l2"]}

gs_lr = GridSearchCV(LR,
                     param_grid = lr_param_grid,
                     cv=kfold,
                     scoring="accuracy",
                     n_jobs=4,
                     verbose = 1)

gs_lr.fit(X_train,y_train)

lr_best = gs_lr.best_estimator_

gs_lr.best_score_
```
    [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.

    Fitting 10 folds for each of 20 candidates, totalling 200 fits

    [Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    1.8s finished

    0.8320090805902384
    
****
<a id="Section_10"></a>
## 10. Ensemble
  

```python
test_Survived_extratree = pd.Series(extraTrees_best.predict(test), name="ExtraTrees")
test_Survived_svc = pd.Series(svc_best.predict(test), name="SVC")
test_Survived_rdf = pd.Series(rdf_best.predict(test), name="RdmForest")
test_Survived_LDA = pd.Series(LDA_best.predict(test), name="LDA")
test_Survived_lr = pd.Series(lr_best.predict(test), name="LR")

# Concatenate all classifier results
ensemble_results = pd.concat([test_Survived_extratree,
                              test_Survived_svc,
                              test_Survived_rdf,
                              test_Survived_LDA,
                              test_Survived_lr],axis=1)

# Generate a mask for the upper triangle
mask = np.zeros_like(ensemble_results.corr(), dtype=np.bool)
mask[np.triu_indices_from(mask)] = True


g= sns.heatmap(ensemble_results.corr(),annot=True, mask=mask)
```


<figure>
    <img src="https://tdody.github.io/assets/img/2019-07-01-Titanic/output_170_0.png">
</figure>



```python
votingC = VotingClassifier(estimators=[('ExtraTress', extraTrees_best),
                                       ('SVC',svc_best),
                                       ('RdmForest',rdf_best),
                                       ('LDA',LDA_best),
                                       ('LR',lr_best)],
                           voting='soft', n_jobs=4)

votingC = votingC.fit(X_train, y_train)
```

****
<a id="Section_11"></a>
## 11. Create Submission


```python
# make prediction on test set
y_test_pred = votingC.predict(test).astype(int)
```


```python
# generate submission file 
submission_df = pd.DataFrame({ 'PassengerId': passengerId,
                            'Survived': y_test_pred})
submission_df.to_csv("voting_submission_df.csv", index=False)
```

The submission leads to a 78.95% accuracy. This is puts the prediction in the top 10% of the Kaggle leaderboard.
